{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ArmyTankClassifier.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "mount_file_id": "1RiSpzMRRuCgqNs10Y1gBJvM6rsECCP3m",
      "authorship_tag": "ABX9TyOFeeUNT8n0I0OFVAligUEM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chris-kehl/BattleTankImageRecognition/blob/master/ArmyTankClassifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nso_IBKAHET6",
        "colab_type": "text"
      },
      "source": [
        "The purpose of this project is to use transfer learning for large image classification to detect three different sets of main battle tanks which are used by the U.S military, the Russian military, and the German military forces.\n",
        "\n",
        "The first step is to arrange our data into folders. We create a train folder, a validation  folder, and a test folder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VlYYdk3hrCK5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os, shutil"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MRzkV2rx8PFL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "original_abrams_dataset_dir = '/content/drive/My Drive/data_files/data/abrams'\n",
        "original_leopard_dataset_dir = '/content/drive/My Drive/data_files/data/leopard'\n",
        "original_t90_dataset_dir = '/content/drive/My Drive/data_files/data/t90'\n",
        "\n",
        "base_dir = '/content/drive/My Drive/data_files/tanks'\n",
        "os.mkdir(base_dir)\n",
        "\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "os.mkdir(train_dir)\n",
        "validation_dir = os.path.join(base_dir, 'validation')\n",
        "os.mkdir(validation_dir)\n",
        "test_dir = os.path.join(base_dir, 'test')\n",
        "os.mkdir(test_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vb9pOtmINur2",
        "colab_type": "text"
      },
      "source": [
        "In the next block of code we are going to build out train, validation, and test folders. Each folder will consist of a subfolder for each tank image. Each subfolder will contain the pictures of the photos of the tanks folder label."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bTRdr87HD2u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_us_dir = os.path.join(train_dir, 'US')\n",
        "os.mkdir(train_us_dir)\n",
        "\n",
        "train_foreign_dir = os.path.join(train_dir, 'Foreign')\n",
        "os.mkdir(train_foreign_dir)\n",
        "\n",
        "validation_us_dir = os.path.join(validation_dir, 'US')\n",
        "os.mkdir(validation_us_dir)\n",
        "\n",
        "validation_foreign_dir = os.path.join(validation_dir, 'Foreign')\n",
        "os.mkdir(validation_foreign_dir)\n",
        "\n",
        "test_us_dir = os.path.join(test_dir, 'US')\n",
        "os.mkdir(test_us_dir)\n",
        "\n",
        "test_foreign_dir = os.path.join(test_dir, 'Foreign')\n",
        "os.mkdir(test_foreign_dir)\n",
        "\n",
        "\n",
        "fnames = ['abrams_{}.jpg'.format(i) for i in range(60)]\n",
        "for fname in fnames:\n",
        "  scr = os.path.join(original_abrams_dataset_dir, fname)\n",
        "  dst = os.path.join(train_us_dir, fname)\n",
        "  shutil.copyfile(scr, dst)\n",
        "\n",
        "fnames = ['abrams_{}.jpg'.format(i) for i in range(60, 80)]\n",
        "for fname in fnames:\n",
        "  scr = os.path.join(original_abrams_dataset_dir, fname)\n",
        "  dst = os.path.join(validation_us_dir, fname)\n",
        "  shutil.copyfile(scr, dst)\n",
        "\n",
        "fnames = ['abrams_{}.jpg'.format(i) for i in range(80, 100)]\n",
        "for fname in fnames:\n",
        "  scr = os.path.join(original_abrams_dataset_dir, fname)\n",
        "  dst = os.path.join(test_us_dir, fname)\n",
        "  shutil.copyfile(scr, dst)\n",
        "\n",
        "fnames = ['leopard_{}.jpg'.format(i) for i in range(60)]\n",
        "for fname in fnames:\n",
        "  scr = os.path.join(original_leopard_dataset_dir, fname)\n",
        "  dst = os.path.join(train_foreign_dir, fname)\n",
        "  shutil.copyfile(scr, dst)\n",
        "\n",
        "fnames = ['leopard_{}.jpg'.format(i) for i in range(60, 80)]\n",
        "for fname in fnames:\n",
        "  scr = os.path.join(original_leopard_dataset_dir, fname)\n",
        "  dst = os.path.join(validation_foreign_dir, fname)\n",
        "  shutil.copyfile(scr, dst)\n",
        "\n",
        "fnames = ['leopard_{}.jpg'.format(i) for i in range(80, 100)]\n",
        "for fname in fnames:\n",
        "  scr = os.path.join(original_leopard_dataset_dir, fname)\n",
        "  dst = os.path.join(test_foreign_dir, fname)\n",
        "  shutil.copyfile(scr, dst)\n",
        "\n",
        "fnames = ['t90_{}.jpg'.format(i) for i in range(60)]\n",
        "for fname in fnames:\n",
        "  scr = os.path.join(original_t90_dataset_dir, fname)\n",
        "  dst = os.path.join(train_foreign_dir, fname)\n",
        "  shutil.copyfile(scr, dst)\n",
        "\n",
        "fnames = ['t90_{}.jpg'.format(i) for i in range(60, 80)]\n",
        "for fname in fnames:\n",
        "  scr = os.path.join(original_t90_dataset_dir, fname)\n",
        "  dst = os.path.join(validation_foreign_dir, fname)\n",
        "  shutil.copyfile(scr, dst)\n",
        "\n",
        "fnames = ['t90_{}.jpg'.format(i) for i in range(80, 100)]\n",
        "for fname in fnames:\n",
        "  scr = os.path.join(original_t90_dataset_dir, fname)\n",
        "  dst = os.path.join(test_foreign_dir, fname)\n",
        "  shutil.copyfile(scr, dst)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QwNSLdMzOTvT",
        "colab_type": "text"
      },
      "source": [
        "We will take a look at our folders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-lJQK6cOX3Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('total training us images:', len(os.listdir(train_us_dir)))\n",
        "print('total training foreign images:', len(os.listdir(train_foreign_dir)))\n",
        "print('total validation us images:', len(os.listdir(validation_us_dir)))\n",
        "print('total validation foreign images:', len(os.listdir(validation_foreign_dir)))\n",
        "print('total test us images:', len(os.listdir(test_us_dir)))\n",
        "print('total test foreign images:', len(os.listdir(test_foreign_dir)))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9DaqcqPSudh",
        "colab_type": "text"
      },
      "source": [
        "Building our network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Kdx8sFwRMr-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import layers\n",
        "from keras import models\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu',\n",
        "                        input_shape=(150, 150, 3)))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(512, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_CpOcItVCvY",
        "colab_type": "text"
      },
      "source": [
        "Configure the model for training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nnRHtL_DUUKq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import optimizers\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=optimizers.RMSprop(lr=1e-4),\n",
        "              metrics=['acc'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fXptbKYVIm-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(150, 150),\n",
        "    batch_size=1,\n",
        "    class_mode='binary')\n",
        "\n",
        "validation_generator = test_datagen.flow_from_directory(\n",
        "    validation_dir,\n",
        "    target_size=(150, 150),\n",
        "    batch_size=1,\n",
        "    class_mode='binary')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3AmpcG8YSgP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for data_batch, labels_batch in train_generator:\n",
        "  print('data batch shape:', data_batch.shape)\n",
        "  print('label batch shape:', labels_batch.shape)\n",
        "  break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6Qo5fE_ZYRc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history = model.fit_generator(\n",
        "    train_generator,\n",
        "    steps_per_epoch=100,\n",
        "    epochs=3,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=50)\n",
        "\n",
        "model.save('tanks_1.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U605PHDAUOFe",
        "colab_type": "text"
      },
      "source": [
        "Lets plot the training and validation accuracy to see if there is any overfitting going on.  Looking at the validaion accuaracy we are going to have to do more tweaking of our model to bring up the validation accuracy percentages."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBt1Zn8XUuax",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label = 'Validation acc')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and Validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsGPOYgBX8Rf",
        "colab_type": "text"
      },
      "source": [
        "Add data augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1VYptlJNX6M2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=40,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3QbgbzxZIiz",
        "colab_type": "text"
      },
      "source": [
        "Looking at the images that we just augmented"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCaoxQ1CZMLT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing import image\n",
        "\n",
        "fnames = [os.path.join(train_us_dir, fname) for\n",
        "          fname in os.listdir(train_us_dir)]\n",
        "\n",
        "img_path = fnames[3]\n",
        "\n",
        "img = image.load_img(img_path, target_size=(150, 150))\n",
        "\n",
        "x = image.img_to_array(img)\n",
        "x = x.reshape((1,) + x.shape)\n",
        "\n",
        "i = 0\n",
        "\n",
        "for batch in datagen.flow(x, batch_size=1):\n",
        "  plt.figure(i)\n",
        "  imgplot = plt.imshow(image.array_to_img(batch[0]))\n",
        "  i += 1\n",
        "  if i % 4 == 0:\n",
        "    break\n",
        "\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cyDjewmiwOn6",
        "colab_type": "text"
      },
      "source": [
        "To improve our model, we will define a new convolutional network model that will include dropout."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LL3Xi2JHycdL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu',\n",
        "          input_shape=(150, 150, 3)))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dropout(0.5))\n",
        "model.add(layers.Dense(512, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='softmax'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=optimizers.RMSprop(lr=1e-4),\n",
        "              metrics=['acc'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dwJcn9l2wDb",
        "colab_type": "text"
      },
      "source": [
        "Train the model network using data augmentation and dropout."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGr7Zh9w26-d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1.255,\n",
        "    rotation_range=40,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest')\n",
        "\n",
        "test_generator = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(150, 150),\n",
        "    batch_size=32,\n",
        "    class_mode='binary')\n",
        "\n",
        "validation_generator = test_datagen.flow_from_directory(\n",
        "    validation_dir,\n",
        "    target_size=(150, 150),\n",
        "    batch_size=32,\n",
        "    class_mode='binary')\n",
        "\n",
        "history = model.fit_generator(\n",
        "    train_generator,\n",
        "    steps_per_epoch=100,\n",
        "    epochs=3,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=50)\n",
        "\n",
        "model.save('tanks_2h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzidWhONsrmE",
        "colab_type": "text"
      },
      "source": [
        "Use the VGG16 architecture to use a pretrained CNN\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fgecb_GSs7XV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.applications import VGG16\n",
        "\n",
        "conv_base = VGG16(weights='imagenet',\n",
        "                  include_top=False,\n",
        "                  input_shape=(150, 150, 3))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sf4SodtqxEeE",
        "colab_type": "text"
      },
      "source": [
        "The above Keras CNN resulted in 72% accuracy without data augmentation and with data augmentation it was worse. I am going to implement fast feature extraction without data augmentation to see how that model performs.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lf_nlo5Aw4aL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "base_dir = '/content/drive/My Drive/data_files/tanks'\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "validation_dir = os.path.join(base_dir, 'validation')\n",
        "test_dir = os.path.join(base_dir, 'test')\n",
        "\n",
        "datagen = ImageDataGenerator(rescale=1./255)\n",
        "batch_size = 5\n",
        "\n",
        "def extract_features(directory, sample_count):\n",
        "  features = np.zeros(shape=(sample_count, 4, 4, 512))\n",
        "  labels = np.zeros(shape=(sample_count))\n",
        "  generator = datagen.flow_from_directory(\n",
        "      directory,\n",
        "      target_size=(150, 150),\n",
        "      batch_size=batch_size,\n",
        "      class_mode='binary')\n",
        "  \n",
        "  i = 0\n",
        "  for inputs_batch, labels_batch in generator:\n",
        "    features_batch = conv_base.predict(inputs_batch)\n",
        "    features[i * batch_size : (i + 1) * batch_size] = features_batch\n",
        "    labels[i * batch_size : (i + 1) * batch_size] = labels_batch\n",
        "    i + 1\n",
        "    if i * batch_size >= sample_count:\n",
        "      break\n",
        "    return features, labels\n",
        "\n",
        "train_features, train_labels = extract_features(train_dir, 180)\n",
        "validation_features, validation_labels = extract_features(validation_dir, 60)\n",
        "test_features, test_labels = extract_features(test_dir, 60)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "239BRw3m2afW",
        "colab_type": "text"
      },
      "source": [
        "The current shape of extracted features are (samples, 4, 4, 512) I will flatten the features in order to feed the features into the densly connected classifier.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cyfw-_2827uq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_features = np.reshape(train_features, (180, 4 * 4 * 512))\n",
        "validation_features = np.reshape(validation_features, (60, 4 * 4 * 512))\n",
        "test_features = np.reshape(test_features, (60, 4 * 4 * 512))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Z9ZtmgD6kYb",
        "colab_type": "text"
      },
      "source": [
        "Define and train the densley connected classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HnZLA44G6hgm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import models\n",
        "from keras import layers\n",
        "from keras import optimizers\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(256, activation='relu', input_dim=4 * 4 * 512))\n",
        "model.add(layers.Dropout(0.5))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer=optimizers.RMSprop(lr=2e-5),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['acc'])\n",
        "\n",
        "histroy = model.fit(train_features, train_labels,\n",
        "                    epochs=5,\n",
        "                    batch_size=1,\n",
        "                    validation_data=(validation_features, validation_labels))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZcnVTzHn8viv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='validation acc')\n",
        "plt.title('Training and Validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='validation acc')\n",
        "plt.title('Training and Validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSqMAl2zAqlq",
        "colab_type": "text"
      },
      "source": [
        "* implementing feature extraction with data augmentation\n",
        "* add a densley connected classifier on top of the convolutional base\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_Qb8gNXAwj5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import models\n",
        "from keras import layers\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(conv_base)\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(256, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttTZS9-CB0XJ",
        "colab_type": "text"
      },
      "source": [
        "At this point we will need to freeze the network to prevent the weights from being updated during training.  If we don't freeze the weights, the previous learned representations will be destroyed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-nLMULoCR2L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('This is the number of trainable weights '\n",
        "      'before freezing the conv base:', len(model.trainable_weights))\n",
        "\n",
        "conv_base.trainable = False\n",
        "\n",
        "print('This is the number of trainable weights '\n",
        "      'after freezing the conv base:', len(model.trainable_weights))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ioACctEEE2E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras import optimizers\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1.255,\n",
        "    rotation_range=40,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest')\n",
        "\n",
        "\n",
        "test_generator = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(150, 150),\n",
        "    batch_size=5,\n",
        "    class_mode='binary')\n",
        "\n",
        "validation_generator = test_datagen.flow_from_directory(\n",
        "    validation_dir,\n",
        "    target_size=(150, 150),\n",
        "    batch_size=1,\n",
        "    class_mode='binary')\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=optimizers.RMSprop(lr=2e-5),\n",
        "              metrics=['acc'])\n",
        "\n",
        "\n",
        "history = model.fit_generator(\n",
        "    train_generator,\n",
        "    steps_per_epoch=100,\n",
        "    epochs=5,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=50)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqik7aGCJwyW",
        "colab_type": "text"
      },
      "source": [
        "Add fine tuning to to our model.  We will freeze some layers and unfreeze others"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HS3KicRAJju-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "conv_base.trainable = True\n",
        "\n",
        "set_trainable = False\n",
        "for layer in conv_base.layers:\n",
        "  if layer.name == 'block5_conv1':\n",
        "    set_trainable = True\n",
        "  if set_trainable:\n",
        "    layer.trainable = True\n",
        "  else:\n",
        "    layer.trainable = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBhBR6DUKrRS",
        "colab_type": "text"
      },
      "source": [
        "To finish our fine tuning we will train the model using a very RMSprop low learning rate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6RBLt08fKozs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=optimizers.RMSprop(lr=1e-5),\n",
        "              metrics=['acc'])\n",
        "\n",
        "history = model.fit_generator(\n",
        "    train_generator,\n",
        "    steps_per_epoch=100,\n",
        "    epochs=5,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=50)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKPcZBHGLoIs",
        "colab_type": "text"
      },
      "source": [
        "Plotting the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-O09qxQILrXX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def smooth_curve(points, factor=0.8):\n",
        "  smoothed_points = []\n",
        "  for point in points:\n",
        "    if smoothed_points:\n",
        "     previous = smoothed_points[-1]\n",
        "     smoothed_points.append(previous * factor + point * (1 - factor))\n",
        "    else:\n",
        "      smoothed_points.append(point)\n",
        "  return smoothed_points\n",
        "\n",
        "plt.plot(epochs,\n",
        "         smooth_curve(acc), 'bo', label='Smoothed training acc')\n",
        "plt.plot(epochs,\n",
        "         smooth_curve(val_acc), 'b', label='Smoothed validation acc')\n",
        "plt.title('Training and Validation accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs,\n",
        "         smooth_curve(loss), 'bo', label='Smoothed training loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhJKGoktNoGW",
        "colab_type": "text"
      },
      "source": [
        "Time to Evaluate the model we build"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YzZOlQJiNupJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_generator = test_datagen.flow_from_directory(\n",
        "    test_dir,\n",
        "    target_size=(150, 150),\n",
        "    batch_size=1,\n",
        "    class_mode='binary')\n",
        "\n",
        "test_loss, test_acc = model.evaluate_generator(test_generator, steps=50)\n",
        "print('test acc:', test_acc)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}